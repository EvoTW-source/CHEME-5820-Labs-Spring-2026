{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b6a48b",
   "metadata": {},
   "source": [
    "# Activity: Linear Discriminant Analysis (LDA) using SVD\n",
    "In this activity, we'll use Linear Discriminant Analysis (LDA) to build a supervised low-dimensional representation of a heart disease dataset. Unlike PCA, LDA explicitly uses class labels to find directions that separate classes, and we'll compute those directions using singular value decomposition (SVD).\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this activity, you should be able to:\n",
    "> \n",
    "> * __Compute class statistics and scatter matrices from clinical data:__ Build within-class and between-class scatter matrices using labeled patient data.\n",
    "> * __Use SVD to solve the LDA optimization problem:__ Convert the generalized eigenvalue problem into a standard eigenvalue problem via SVD-based whitening.\n",
    "> * __Project and interpret class separation in a reduced space:__ Visualize LDA projections and assess class separation for the `death_event` label.\n",
    "\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cddff4",
   "metadata": {},
   "source": [
    "## Review: Supervised Dimensionality Reduction with LDA\n",
    "Suppose we have a dataset $\\mathcal{D} = \\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$ where $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$ is a feature vector and $y_{i}\\in\\{-1,1\\}$ is a class label. LDA seeks a projection vector $\\mathbf{w}$ that maximizes separation between classes while minimizing spread within each class.\n",
    "\n",
    "> __Fisher criterion and generalized eigenproblem:__ LDA chooses $\\mathbf{w}$ to maximize the ratio of between-class scatter to within-class scatter:\n",
    "> $$\n",
    "> J(\\mathbf{w}) = \\frac{\\mathbf{w}^{\\top}\\mathbf{S}_{b}\\,\\mathbf{w}}{\\mathbf{w}^{\\top}\\mathbf{S}_{w}\\,\\mathbf{w}}\n",
    "> $$\n",
    "> Maximizing $J(\\mathbf{w})$ yields the generalized eigenvalue problem:\n",
    "> $$\n",
    "> \\mathbf{S}_{b}\\,\\mathbf{w} = \\lambda\\,\\mathbf{S}_{w}\\,\\mathbf{w}\n",
    "> $$\n",
    "> where:\n",
    "> * $J(\\mathbf{w})$ is the Fisher criterion value (a scalar).\n",
    "> * $\\mathcal{D} = \\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$ is the labeled dataset with $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$ and $y_{i}\\in\\{-1,1\\}$.\n",
    "> * $\\mathbf{w}\\in\\mathbb{R}^{m}$ is the projection direction.\n",
    "> * $\\mathbf{S}_{w}$ is the within-class scatter matrix and $\\mathbf{S}_{b}$ is the between-class scatter matrix.\n",
    "> * $\\lambda$ is the generalized eigenvalue associated with $\\mathbf{w}$.\n",
    "\n",
    "Because this lab follows a lecture on SVD, we'll solve this problem using SVD-based whitening rather than explicit matrix inversion.\n",
    "\n",
    "> __TL;DR.__ LDA is a supervised dimensionality reduction method that finds directions maximizing class separation, and SVD gives a numerically stable way to compute those directions.\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b0f87",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5e38e",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81318134",
   "metadata": {},
   "source": [
    "### Data\n",
    "Next, let's load up the dataset that we will explore. The data for this lab was taken from this `2020` publication:\n",
    "* [Davide Chicco, Giuseppe Jurman: \"Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone.\" BMC Medical Informatics and Decision Making 20, 16 (2020). https://doi.org/10.1186/s12911-020-1023-5](https://pubmed.ncbi.nlm.nih.gov/32013925/)\n",
    "\n",
    "In this paper, the authors analyzed a dataset of 299 heart failure patients collected in 2015. The patients comprised 105 women and 194 men, aged between 40 and 95 years old. The dataset contains 13 features (a mixture of continuous and categorical data), which report clinical, body, and lifestyle information:\n",
    "* Some features are binary: anemia, high blood pressure, diabetes, sex, and smoking status.\n",
    "* The remaining features were continuous biochemical measurements, such as the level of the Creatinine phosphokinase (CPK) enzyme in the blood, the number of platelets, etc.\n",
    "* The class (target) variable is encoded as a binary (boolean) death event: `1` if the patient died during the follow-up period, `0` if the patient did not die during the follow-up period.\n",
    "\n",
    "We'll load this dataset as a [DataFrame instance](https://dataframes.juliadata.org/stable/) and store it in the `originaldataset::DataFrame` variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "originaldataset = MyHeartDiseaseClinicalDataset(); # load the heart disease dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63478381",
   "metadata": {},
   "source": [
    "#### Data scaling\n",
    "LDA requires [a `Matrix`](https://docs.julialang.org/en/v1/base/arrays/#Base.Matrix-Tuple{UndefInitializer,%20Any,%20Any}), not [a `DataFrame`](https://dataframes.juliadata.org/stable/). We preprocess the data in three steps:\n",
    "\n",
    "> __Data Preprocessing:__\n",
    "> \n",
    "> * __Binary recoding:__ Convert categorical `0,1` data to `-1,1` where `0` maps to `-1` and `1` remains `1`.\n",
    "> * __Z-score normalization:__ Apply [z-score scaling](https://en.wikipedia.org/wiki/Feature_scaling) to continuous features using $x^{\\prime} = (x - \\mu)/\\sigma$ where $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n",
    "> * __Label retention:__ Keep the `death_event` label because LDA is supervised.\n",
    "\n",
    "The preprocessed feature matrix is stored in `X::Matrix{Float64}`, while the label vector is stored in `y::Vector{Float64}`. We also keep the treated dataset in `dataset::DataFrame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y, dataset) = let\n",
    "\n",
    "    # convert 0,1 into -1,1\n",
    "    treated_dataset = copy(originaldataset);\n",
    "    transform!(treated_dataset, :anaemia => ByRow(x -> (x==0 ? -1 : 1)) => :anaemia); # maps anaemia to -1,1\n",
    "    transform!(treated_dataset, :diabetes => ByRow(x -> (x==0 ? -1 : 1)) => :diabetes); # maps diabetes to -1,1\n",
    "    transform!(treated_dataset, :high_blood_pressure => ByRow(x -> (x==0 ? -1 : 1)) => :high_blood_pressure); # maps high_blood_pressure to -1,1\n",
    "    transform!(treated_dataset, :sex => ByRow(x -> (x==0 ? -1 : 1)) => :sex); # maps sex to -1,1\n",
    "    transform!(treated_dataset, :smoking => ByRow(x -> (x==0 ? -1 : 1)) => :smoking); # maps smoking to -1,1\n",
    "    transform!(treated_dataset, :death_event => ByRow(x -> (x==0 ? -1 : 1)) => :death_event); # maps death_event to -1,1\n",
    "    \n",
    "    D = treated_dataset[:,1:end] |> Matrix; # build a data matrix from the DataFrame\n",
    "    (number_of_examples, number_of_features) = size(D);\n",
    "\n",
    "    # Which cols do we want to rescale?\n",
    "    index_to_z_scale = [\n",
    "        1 ; # 1 age\n",
    "        3 ; # 2 creatinine_phosphokinase\n",
    "        5 ; # 3 ejection_fraction\n",
    "        7 ; # 4 platelets\n",
    "        8 ; # 5 serum_creatinine\n",
    "        9 ; # 6 serum_sodium\n",
    "        12 ; # 7 time\n",
    "    ];\n",
    "\n",
    "    D̂ = copy(D);\n",
    "    for i ∈ eachindex(index_to_z_scale)\n",
    "        j = index_to_z_scale[i];\n",
    "        μ = mean(D[:,j]); # compute the mean\n",
    "        σ = std(D[:,j]); # compute std\n",
    "\n",
    "        # rescale -\n",
    "        for k ∈ 1:number_of_examples\n",
    "            D̂[k,j] = (D[k,j] - μ)/σ;\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # split features and labels\n",
    "    X = D̂[:,1:end-1]; # features\n",
    "    y = D̂[:,end]; # labels (death_event)\n",
    "\n",
    "    X, y, treated_dataset\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e2e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b95a2",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d3155",
   "metadata": {},
   "source": [
    "## Task 1: Compute Class Statistics and Scatter Matrices\n",
    "In this task, we'll compute the class means and the scatter matrices used in LDA. We need two types of means: the class-specific mean for each label, and the overall mean across all samples. These means help us measure how spread out each class is internally (within-class scatter) and how far apart the classes are from each other (between-class scatter).\n",
    "\n",
    "> __Class statistics and scatter matrices:__\n",
    ">\n",
    "> First, we compute the class mean $\\mathbf{m}_{c}$ for each class and the overall mean $\\mathbf{m}$:\n",
    "> $$\n",
    "> \\mathbf{m}_{c} = \\frac{1}{n_{c}}\\sum_{i:y_{i}=c} \\mathbf{x}_{i}, \\qquad \\mathbf{m} = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbf{x}_{i}\n",
    "> $$\n",
    "> Using these means, we build the within-class scatter matrix $\\mathbf{S}_{w}$ (measuring how much data varies within each class) and the between-class scatter matrix $\\mathbf{S}_{b}$ (measuring how far apart the class means are):\n",
    "> $$\n",
    "> \\mathbf{S}_{w} = \\sum_{c\\in\\{-1,1\\}}\\;\\sum_{i:y_{i}=c}(\\mathbf{x}_{i}-\\mathbf{m}_{c})(\\mathbf{x}_{i}-\\mathbf{m}_{c})^{\\top}\n",
    "> $$\n",
    "> $$\n",
    "> \\mathbf{S}_{b} = \\sum_{c\\in\\{-1,1\\}} n_{c}(\\mathbf{m}_{c}-\\mathbf{m})(\\mathbf{m}_{c}-\\mathbf{m})^{\\top}\n",
    "> $$\n",
    "> where $c\\in\\{-1,1\\}$ is the class label, $n_{c}$ is the number of samples in class $c$, $n$ is the total number of samples, $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$ is the feature vector for sample $i$, and $y_{i}$ is its class label.\n",
    "\n",
    "Let's compute $\\mathbf{S}_{w}$ and $\\mathbf{S}_{b}$ from the preprocessed data. We'll store the class indices in `index_pos::Vector{Int}` and `index_neg::Vector{Int}`, the class means in `m_pos::Vector{Float64}` and `m_neg::Vector{Float64}`, the overall mean in `m::Vector{Float64}`, and the scatter matrices in `S_w::Matrix{Float64}` and `S_b::Matrix{Float64}`:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0255ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(index_pos, index_neg, m, m_pos, m_neg, S_w, S_b) = let\n",
    "\n",
    "    # split by class\n",
    "    index_pos = findall(==(1), y);\n",
    "    index_neg = findall(==(-1), y);\n",
    "    X_pos = X[index_pos, :];\n",
    "    X_neg = X[index_neg, :];\n",
    "\n",
    "    # means\n",
    "    m = mean(X, dims=1) |> vec;\n",
    "    m_pos = mean(X_pos, dims=1) |> vec;\n",
    "    m_neg = mean(X_neg, dims=1) |> vec;\n",
    "\n",
    "    # within-class scatter\n",
    "    n_features = size(X,2);\n",
    "    S_w = zeros(n_features, n_features);\n",
    "    for i ∈ 1:size(X_pos,1)\n",
    "        x = vec(X_pos[i, :]) - m_pos;\n",
    "        S_w += x * x';\n",
    "    end\n",
    "    for i ∈ 1:size(X_neg,1)\n",
    "        x = vec(X_neg[i, :]) - m_neg;\n",
    "        S_w += x * x';\n",
    "    end\n",
    "\n",
    "    # between-class scatter\n",
    "    n_pos = length(index_pos);\n",
    "    n_neg = length(index_neg);\n",
    "    Δ_pos = m_pos - m;\n",
    "    Δ_neg = m_neg - m;\n",
    "    S_b = n_pos*(Δ_pos * Δ_pos') + n_neg*(Δ_neg * Δ_neg');\n",
    "\n",
    "    index_pos, index_neg, m, m_pos, m_neg, S_w, S_b\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8498061",
   "metadata": {},
   "source": [
    "__Check__: Both scatter matrices should be symmetric. Let's verify this numerically:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73241414",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    ϵ = 1e-8;\n",
    "    test_sw = norm(S_w - S_w', Inf) < ϵ;\n",
    "    test_sb = norm(S_b - S_b', Inf) < ϵ;\n",
    "\n",
    "    @assert test_sw \"S_w is not symmetric within tolerance!\"\n",
    "    @assert test_sb \"S_b is not symmetric within tolerance!\"\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc808e21",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2f11d",
   "metadata": {},
   "source": [
    "## Task 2: Compute the LDA Direction using SVD\n",
    "Maximizing the Fisher criterion leads to a generalized eigenvalue problem. Rather than inverting matrices directly (which can be numerically unstable), we use SVD-based whitening to convert the problem into a standard eigenvalue problem.\n",
    "\n",
    "> __Generalized eigenvalue form:__\n",
    ">\n",
    "> The LDA direction $\\mathbf{w}$ satisfies:\n",
    "> $$\n",
    "> \\mathbf{S}_{b}\\,\\mathbf{w} = \\lambda\\,\\mathbf{S}_{w}\\,\\mathbf{w}\n",
    "> $$\n",
    "> where $\\mathbf{w}\\in\\mathbb{R}^{m}$ is the LDA direction, $\\lambda$ is the generalized eigenvalue, and $\\mathbf{S}_{w}, \\mathbf{S}_{b}$ are the within-class and between-class scatter matrices.\n",
    "\n",
    "To solve this without matrix inversion, we apply SVD-based whitening. We compute the SVD of $\\mathbf{S}_{w}$ to build a whitening transformation that normalizes the within-class scatter, then solve a simpler eigenvalue problem in that transformed space.\n",
    "\n",
    "> __SVD-based whitening:__\n",
    ">\n",
    "> If $\\mathbf{S}_{w} = \\mathbf{U}\\,\\mathbf{\\Sigma}\\,\\mathbf{U}^{\\top}$, then we can construct the inverse square root:\n",
    "> $$\n",
    "> \\mathbf{S}_{w}^{-1/2} = \\mathbf{U}\\,\\mathbf{\\Sigma}^{-1/2}\\,\\mathbf{U}^{\\top}\n",
    "> $$\n",
    "> The generalized eigenvalue problem reduces to a standard eigenvalue problem in the whitened space:\n",
    "> $$\n",
    "> \\mathbf{M} = \\mathbf{S}_{w}^{-1/2}\\,\\mathbf{S}_{b}\\,\\mathbf{S}_{w}^{-1/2}\n",
    "> $$\n",
    "> where $\\mathbf{U}$ contains the left singular vectors of $\\mathbf{S}_{w}$, $\\mathbf{\\Sigma}$ contains the singular values, and $\\mathbf{M}$ is the whitened between-class scatter matrix.\n",
    "\n",
    "We'll compute the dominant eigenvector of $\\mathbf{M}$ using SVD and map it back to the original feature space to obtain the LDA direction $\\mathbf{w}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5804af",
   "metadata": {},
   "outputs": [],
   "source": [
    "(w, λ̂) = let\n",
    "\n",
    "    # SVD of within-class scatter\n",
    "    svd_sw = svd(S_w);\n",
    "    U = svd_sw.U;\n",
    "    Σ = svd_sw.S;\n",
    "\n",
    "    # build S_w^{-1/2} with a small tolerance\n",
    "    ϵ = 1e-10;\n",
    "    Σ_inv_sqrt = Diagonal([s > ϵ ? 1/sqrt(s) : 0.0 for s in Σ]);\n",
    "    S_w_inv_sqrt = U * Σ_inv_sqrt * U';\n",
    "\n",
    "    # whitened between-class scatter\n",
    "    M = S_w_inv_sqrt * S_b * S_w_inv_sqrt;\n",
    "\n",
    "    # dominant direction via SVD\n",
    "    svd_m = svd(M);\n",
    "    v₁ = svd_m.U[:,1];\n",
    "    w = S_w_inv_sqrt * v₁;\n",
    "    w = w / norm(w); # normalize\n",
    "\n",
    "    λ = svd_m.S[1];\n",
    "    w, λ\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72389fb7",
   "metadata": {},
   "source": [
    "__Check__: Let's compute the Fisher criterion value for the LDA direction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    numerator = (w' * S_b * w)[1];\n",
    "    denominator = (w' * S_w * w)[1];\n",
    "    J = numerator/denominator;\n",
    "\n",
    "    println(\"Fisher criterion J(w) = $(round(J, digits=4))\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6263f8df",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe64bc",
   "metadata": {},
   "source": [
    "## Task 3: Visualize the LDA Projection\n",
    "Because this dataset has two classes, LDA produces a single discriminant direction. We'll project each data point onto $\\mathbf{w}$ and visualize the class separation along that axis. By centering the data first (subtracting the overall mean), we ensure the projection captures deviations from the average, which makes class separation clearer.\n",
    "\n",
    "> __LDA projection:__\n",
    ">\n",
    "> For each sample, we compute the scalar LDA score and store all scores in the `Z::Vector{Float64}` variable:\n",
    "> $$\n",
    "> z_{i} = (\\mathbf{x}_{i}-\\mathbf{m})^{\\top}\\mathbf{w}\n",
    "> $$\n",
    "> where $z_{i}$ is the LDA score for sample $i$, $\\mathbf{x}_{i}$ is the feature vector, $\\mathbf{m}$ is the overall mean vector, and $\\mathbf{w}$ is the LDA direction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97589d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = let\n",
    "\n",
    "    # center the data\n",
    "    X_centered = X .- (ones(size(X,1)) * m');\n",
    "    Z = X_centered * w; # projected scores\n",
    "    Z\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d564934",
   "metadata": {},
   "source": [
    "__Visualize__: We'll plot the projected scores for each class and mark the midpoint between class means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb5ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    # projected scores by class\n",
    "    Z_pos = Z[index_pos];\n",
    "    Z_neg = Z[index_neg];\n",
    "\n",
    "    # class means in LDA space\n",
    "    μ_pos = mean(Z_pos);\n",
    "    μ_neg = mean(Z_neg);\n",
    "    τ = (μ_pos + μ_neg)/2; # midpoint\n",
    "\n",
    "    # visualize\n",
    "    histogram(Z_pos; bins=30, alpha=0.5, color=:red, label=\"Death Event\")\n",
    "    histogram!(Z_neg; bins=30, alpha=0.5, color=:navy, label=\"No Death Event\")\n",
    "    vline!([τ], c=:gray40, lw=2, label=\"Midpoint\")\n",
    "\n",
    "    plot!(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent);\n",
    "    xlabel!(\"LDA projection (LD1)\", fontsize=18)\n",
    "    ylabel!(\"Count\", fontsize=18)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eecc10",
   "metadata": {},
   "source": [
    "__What do you observe?__\n",
    "\n",
    "Because LDA explicitly uses class labels, the projection typically shows clearer separation than PCA. In this heart disease dataset, the two classes shift apart along the discriminant axis, but some overlap remains because the clinical features are not perfectly separable.\n",
    "\n",
    "> __Interpretation__: The midpoint line provides a simple visual threshold that highlights where misclassifications are most likely to occur. LDA maximizes class separation in a low-dimensional space, but real clinical data often exhibit overlap due to noise, confounding factors, and feature correlations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518a91b",
   "metadata": {},
   "source": [
    "__Performance__: We'll classify each sample by thresholding the LDA score at the midpoint between class means, then compute a confusion matrix. The results are stored in `CM_lda::Matrix{Int}` (the confusion matrix) and `ŷ_lda::Vector{Int}` (the predicted labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72404fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "(CM_lda, ŷ_lda) = let\n",
    "\n",
    "    # recompute midpoint threshold in LDA space\n",
    "    Z_pos = Z[index_pos];\n",
    "    Z_neg = Z[index_neg];\n",
    "    μ_pos = mean(Z_pos);\n",
    "    μ_neg = mean(Z_neg);\n",
    "    τ = (μ_pos + μ_neg)/2;\n",
    "\n",
    "    # assign labels using the midpoint threshold\n",
    "    ŷ = [z >= τ ? 1 : -1 for z ∈ Z];\n",
    "\n",
    "    # confusion matrix\n",
    "    CM = confusion(Int.(y), Int.(ŷ));\n",
    "    CM, ŷ\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eaa28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    number_of_examples = length(y);\n",
    "    correct_predictions = CM_lda[1,1] + CM_lda[2,2];\n",
    "    (correct_predictions/number_of_examples) |> f -> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b52fd7",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a8f234",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This activity applied Linear Discriminant Analysis to a heart disease dataset and used SVD-based whitening to compute the discriminant direction.\n",
    "\n",
    "> __Key Takeaways:__\n",
    "> \n",
    "> * **LDA is supervised dimensionality reduction:** It uses class labels to find directions that maximize between-class variance while minimizing within-class variance.\n",
    "> * **SVD provides a stable route to LDA solutions:** Whitening the within-class scatter matrix with SVD converts the generalized eigenvalue problem into a standard eigenvalue problem.\n",
    "> * **LDA projections reveal class separation:** Even with overlap, the LDA axis exposes the dominant direction that best separates the `death_event` classes.\n",
    "\n",
    "LDA complements PCA by focusing on class separability rather than variance alone, making it a useful tool when labels are available.\n",
    "\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.4",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
