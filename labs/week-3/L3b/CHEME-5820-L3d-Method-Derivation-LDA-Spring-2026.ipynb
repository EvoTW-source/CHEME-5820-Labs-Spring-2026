{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Derivation: Linear Discriminant Analysis (LDA) via SVD\n",
        "How should we project high-dimensional labeled data onto a lower-dimensional space to best separate the classes? Linear Discriminant Analysis (LDA) answers this by finding directions that maximize between-class variance (spreading class means apart) while minimizing within-class variance (keeping each class compact).\n",
        "\n",
        "> **Learning Objectives:**\n",
        ">\n",
        "> By the end of this lesson, you should be able to:\n",
        "> * Formulate the LDA objective using within-class and between-class scatter matrices\n",
        "> * Derive the generalized eigenvalue problem that maximizes Fisher's criterion\n",
        "> * Develop a numerically stable SVD-based algorithm that avoids explicit matrix inversion\n",
        "\n",
        "Let's get started!\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Theory: Scatter matrices and the generalized eigenvalue problem\n",
        "\n",
        "**Notation and setup.** We have $n$ labeled samples $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, where $\\mathbf{x}_i\\in\\mathbb{R}^{m}$ is a feature vector and $y_i$ is the class label. For each class $c$, let $n_c$ be the number of samples in $\\mathcal{D}_c$ and define the class mean and overall mean:\n",
        "$$\n",
        "\\mathbf{m}_{c} = \\frac{1}{n_c}\\sum_{i\\,\\in\\,\\mathcal{D}_{c}}\\mathbf{x}_i, \\qquad \\mathbf{m} = \\frac{1}{n}\\sum_{i=1}^n\\mathbf{x}_i.\n",
        "$$\n",
        "\n",
        "For the common binary classification case with classes $y_i\\in\\{-1,1\\}$, we partition the data into $\\mathcal{D}_{+} = \\{\\mathbf{x}_i : y_i=1\\}$ and $\\mathcal{D}_{-} = \\{\\mathbf{x}_i : y_i=-1\\}$ with corresponding means $\\mathbf{m}_{+}$ and $\\mathbf{m}_{-}$.\n",
        "\n",
        "**Scatter matrices.** The **within-class scatter** and **between-class scatter** measure two aspects of the data:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbf{S}_w &= \\sum_{c}\\sum_{i\\,\\in\\,\\mathcal{D}_c}(\\mathbf{x}_i-\\mathbf{m}_{c})(\\mathbf{x}_i-\\mathbf{m}_{c})^{\\top},\\\\[4pt]\n",
        "\\mathbf{S}_b &= \\sum_{c} n_c(\\mathbf{m}_{c}-\\mathbf{m})(\\mathbf{m}_{c}-\\mathbf{m})^{\\top}.\n",
        "\\end{align*}\n",
        "$$\n",
        "$\\mathbf{S}_w$ quantifies within-class covariance (how tight each class is), while $\\mathbf{S}_b$ quantifies between-class separation (how spread apart class means are). Together: $\\mathbf{S}_t = \\mathbf{S}_w + \\mathbf{S}_b$.\n",
        "\n",
        "**Fisher's criterion.** To find a projection direction $\\mathbf{w}$ that best separates classes, we look for one-dimensional projections $z = \\mathbf{w}^{\\top}\\mathbf{x}$ that __maximizes__ the ratio (Fisher's criterion):\n",
        "$$\n",
        "J(\\mathbf{w}) = \\frac{\\mathbf{w}^{\\top}\\mathbf{S}_b\\mathbf{w}}{\\mathbf{w}^{\\top}\\mathbf{S}_w\\mathbf{w}}.\n",
        "$$\n",
        "\n",
        "To find the optimal $\\mathbf{w}$, impose the constraint $\\mathbf{w}^{\\top}\\mathbf{S}_w\\mathbf{w}=1$ (normalization with respect to within-class scatter) and use Lagrange multipliers:\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{w},\\lambda)=\\mathbf{w}^{\\top}\\mathbf{S}_b\\mathbf{w}-\\lambda(\\mathbf{w}^{\\top}\\mathbf{S}_w\\mathbf{w}-1).\n",
        "$$\n",
        "where $\\mathcal{L}(\\cdot)$ is the Lagrangian function and $\\lambda$ is the Lagrange multiplier. Taking the gradient with respect to $\\mathbf{w}$ and setting $\\nabla_\\mathbf{w}\\mathcal{L}=0$ gives:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla_\\mathbf{w}\\mathcal{L} &= (\\mathbf{S}_b + \\mathbf{S}_b^{\\top})\\mathbf{w} - \\lambda(\\mathbf{S}_w + \\mathbf{S}_w^{\\top})\\mathbf{w}\\quad(\\mathbf{S}_{b}\\;\\text{and}\\mathbf{S}_{w}\\;\\text{symmetric}) \\\\\n",
        "&= 2\\mathbf{S}_b\\mathbf{w} - 2\\lambda\\mathbf{S}_w\\mathbf{w}\\\\\n",
        "&= \\mathbf{S}_b\\mathbf{w} - \\lambda\\mathbf{S}_w\\mathbf{w}\n",
        "\\end{align*}\n",
        "$$\n",
        "the __generalized eigenvalue problem__:\n",
        "$$\n",
        "\\boxed{\\mathbf{S}_b\\mathbf{w} = \\lambda\\mathbf{S}_w\\mathbf{w}},\n",
        "$$\n",
        "At the optimum, the generalized eigenvalue $\\lambda$ equals the maximum Fisher criterion value: $\\lambda_{\\max} = \\max J(\\mathbf{w})$.\n",
        "\n",
        "**Multiple discriminant directions.** For multi-class (beyond binary) problems, one direction is rarely sufficient. We extend the criterion by collecting $r$ projection vectors as $\\mathbf{W}=[\\mathbf{w}_1,\\dots,\\mathbf{w}_r]\\in\\mathbb{R}^{m\\times r}$ and optimizing:\n",
        "$$\n",
        "J(\\mathbf{W}) = \\mathrm{tr}\\left[\\left(\\mathbf{W}^{\\top}\\mathbf{S}_w\\mathbf{W}\\right)^{-1}\\left(\\mathbf{W}^{\\top}\\mathbf{S}_b\\mathbf{W}\\right)\\right].\n",
        "$$\n",
        "\n",
        "The optimal columns still satisfy the same generalized eigenvalue equation, solved sequentially: $\\mathbf{w}_1$ is the top eigenvector, $\\mathbf{w}_2$ the next, and so on.\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing LDA: The SVD approach\n",
        "\n",
        "We've now formulated the problem: find eigenvectors of $\\mathbf{S}_b\\mathbf{w} = \\lambda\\mathbf{S}_w\\mathbf{w}$. A naive approach is to invert the matrix $\\mathbf{S}_w$: which allows us to rewrite the generalized eigenvalue problem as $(\\mathbf{S}_w^{-1}\\mathbf{S}_b)\\mathbf{w} = \\lambda\\mathbf{w}$ and compute eigenvectors. \n",
        "\n",
        "> __The wrinkle__: But this fails when $\\mathbf{S}_w$ is singular (which happens whenever $m > n$ or when classes are low-rank). Direct inversion amplifies numerical errors and may not even be possible.\n",
        "\n",
        "Instead, we use **SVD to construct a stable whitening transform**. This replaces the ill-conditioned inversion with a well-conditioned change of variables.\n",
        "\n",
        "### Intuition: Whitening\n",
        "Imagine your data classes look like elongated ellipsoids, some directions have high variance, others low. The within-class scatter $\\mathbf{S}_w$ encodes this shape. **The core insight**: if we could __reshape these ellipsoids__ into spheres (all directions having equal variance), the problem becomes much simpler.\n",
        "\n",
        "> **What whitening does geometrically:**\n",
        ">\n",
        "> - **Before whitening**: Classes might be stretched along certain axes, compressed along others. Finding the best separation direction must account for this unequal spread.\n",
        "> - **After whitening**: We transform the space so that within-class variance becomes uniform in all directions—like inflating a football into a basketball. Now we can focus purely on where the class centers are, not how the classes are shaped.\n",
        "\n",
        "Whitening normalizes out the within-class covariance structure. Once the within-class scatter looks like an identity matrix (spherical), the generalized eigenvalue problem $\\mathbf{S}_b\\mathbf{w} = \\lambda\\mathbf{S}_w\\mathbf{w}$ reduces to a standard one: just find directions where the between-class scatter is largest.\n",
        "\n",
        "**Why SVD?** The SVD gives us the natural coordinate system for this transformation, it reveals the principal directions of within-class variance and how much to scale each one to make the scatter spherical.\n",
        "\n",
        "### Mathematics: How We Whiten\n",
        "\n",
        "**Form the centered data matrix.** Collect all data points centered by their class mean:\n",
        "$$\n",
        "\\mathbf{H} = [\\mathbf{x}_1-\\mathbf{m}_{y_1},\\dots,\\mathbf{x}_n-\\mathbf{m}_{y_n}]\\in\\mathbb{R}^{m\\times n}.\n",
        "$$\n",
        "Then $\\mathbf{S}_w = \\mathbf{H}\\mathbf{H}^{\\top}$ follows directly from the definition.\n",
        "\n",
        "**Decompose via thin SVD.** Now compute\n",
        "$$\n",
        "\\mathbf{H} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\top},\\qquad \\mathbf{U}\\in\\mathbb{R}^{m\\times r_w},\\quad \\mathbf{\\Sigma}\\in\\mathbb{R}^{r_w\\times r_w},\\quad r_w = \\mathrm{rank}(\\mathbf{H}).\n",
        "$$\n",
        "This gives us $\\mathbf{S}_w = \\mathbf{U}\\mathbf{\\Sigma}^2\\mathbf{U}^{\\top}$. The key insight: $\\mathbf{U}$ are the principal axes of within-class variance, and $\\mathbf{\\Sigma}^2$ tells us how much variance along each axis.\n",
        "\n",
        "**Whiten via a change of variables.** The whitening transformation is $\\mathbf{Z} = \\mathbf{\\Sigma}^{-1}\\mathbf{U}^{\\top}$. It rotates to principal axes ($\\mathbf{U}^{\\top}$) and rescales each axis ($\\mathbf{\\Sigma}^{-1}$) to unit variance. In the whitened space, within-class scatter becomes the identity: $\\mathbf{Z}\\mathbf{S}_w\\mathbf{Z}^{\\top} = \\mathbf{I}$.\n",
        "\n",
        "Instead of working with $\\mathbf{w}$ directly, define a new variable $\\mathbf{a}$ that lives in the whitened space, connected via $\\mathbf{w}=\\mathbf{U}\\mathbf{\\Sigma}^{-1}\\mathbf{a}$. Substituting this into the generalized eigenvalue problem $\\mathbf{S}_b\\mathbf{w} = \\lambda\\mathbf{S}_w\\mathbf{w}$ gives:\n",
        "$$\n",
        "\\mathbf{S}_b(\\mathbf{U}\\mathbf{\\Sigma}^{-1}\\mathbf{a}) = \\lambda(\\mathbf{U}\\mathbf{\\Sigma}^2\\mathbf{U}^{\\top})(\\mathbf{U}\\mathbf{\\Sigma}^{-1}\\mathbf{a}).\n",
        "$$\n",
        "Simplifying (the $\\mathbf{U}^{\\top}\\mathbf{U}$ terms cancel, the $\\mathbf{\\Sigma}$ terms combine), we obtain a standard eigenvalue problem in the whitened space:\n",
        "$$\n",
        "\\boxed{(\\mathbf{\\Sigma}^{-1}\\mathbf{U}^{\\top}\\mathbf{S}_b\\mathbf{U}\\mathbf{\\Sigma}^{-1})\\mathbf{a} = \\lambda\\mathbf{a}}.\n",
        "$$\n",
        "The matrix $\\mathbf{\\Sigma}^{-1}\\mathbf{U}^{\\top}\\mathbf{S}_b\\mathbf{U}\\mathbf{\\Sigma}^{-1}$ is the between-class scatter _after whitening_—it shows class separation in the space where within-class variance is spherical.\n",
        "\n",
        "**Recover the original directions.** Once we find the eigenvectors $\\mathbf{a}$ of this whitened problem, we transform back to the original space:\n",
        "$$\n",
        "\\mathbf{w} = \\mathbf{U}\\mathbf{\\Sigma}^{-1}\\mathbf{a}.\n",
        "$$\n",
        "This reverses the whitening, giving us discriminant directions in the original coordinate system.\n",
        "\n",
        "> __Numerical note:__\n",
        ">\n",
        "> If $\\mathbf{S}_w$ is singular, use the pseudoinverse $\\mathbf{\\Sigma}^{+}$ (keep only singular values above a tolerance). This makes the LDA solution well-defined and avoids explicit inversion.\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVD-based LDA algorithm\n",
        "__Initialization__: Given labeled data $\\{(\\mathbf{x}_i,y_i)\\}_{i=1}^n$ with classes, compute class means $\\mathbf{m}_{c}$, overall mean $\\mathbf{m}$, and class counts $n_c$. Choose a nonnegative tolerance $\\tau$ for truncating small singular values (e.g., $\\tau\\approx 0$ or a small multiple of machine precision).\n",
        "\n",
        "- Build the within-class centered matrix $\\mathbf{H}=[\\mathbf{x}_i-\\mathbf{m}_{y_i}]$.\n",
        "- Compute the thin SVD: $\\mathbf{H}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\top}$.\n",
        "- Form the between-class mean matrix $\\mathbf{M}=[\\sqrt{n_1}(\\mathbf{m}_{1}-\\mathbf{m}),\\dots,\\sqrt{n_k}(\\mathbf{m}_{k}-\\mathbf{m})]$ so that $\\mathbf{S}_b=\\mathbf{M}\\mathbf{M}^{\\top}$.\n",
        "- Whiten with SVD: $\\mathbf{B}=\\mathbf{\\Sigma}^{-1}\\mathbf{U}^{\\top}\\mathbf{M}$ (use $\\mathbf{\\Sigma}^{+}$ and discard singular values $<\\tau$).\n",
        "- Solve the reduced eigenproblem: $\\mathbf{B}\\mathbf{B}^{\\top}\\mathbf{a}=\\lambda\\mathbf{a}$ and order eigenpairs by descending $\\lambda$.\n",
        "- Recover discriminant directions: $\\mathbf{w}=\\mathbf{U}\\mathbf{\\Sigma}^{-1}\\mathbf{a}$ and keep the top $r$ vectors.\n",
        "\n",
        "__Output__: Return the LDA projection matrix $\\mathbf{W}_r=[\\mathbf{w}_1,\\dots,\\mathbf{w}_r]$ and the associated eigenvalues $\\lambda_1\\ge\\cdots\\ge\\lambda_r$.\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "LDA reduces to a generalized eigenvalue problem that can be solved stably with SVD.\n",
        "\n",
        "> __Key Takeaways:__\n",
        ">\n",
        "> * LDA maximizes Fisher's criterion—the ratio of between-class to within-class scatter—yielding the generalized eigenvalue problem $\\mathbf{S}_b\\mathbf{w}=\\lambda\\mathbf{S}_w\\mathbf{w}$, where at the optimum $\\lambda_{\\max} = \\max J(\\mathbf{w})$.\n",
        "> * The thin SVD of the within-class centered data matrix $\\mathbf{H}$ provides $\\mathbf{S}_w = \\mathbf{U}\\mathbf{\\Sigma}^2\\mathbf{U}^{\\top}$, enabling a numerically stable solution that naturally handles singular scatter matrices without explicit inversion.\n",
        "> * Whitening via $\\mathbf{Z}=\\mathbf{\\Sigma}^{-1}\\mathbf{U}^{\\top}$ transforms the generalized eigenproblem into a standard eigenproblem in reduced space: $\\left(\\mathbf{\\Sigma}^{-1}\\mathbf{U}^{\\top}\\mathbf{S}_b\\mathbf{U}\\mathbf{\\Sigma}^{-1}\\right)\\mathbf{a} = \\lambda\\mathbf{a}$.\n",
        "\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia 1.12",
      "language": "julia",
      "name": "julia-1.12"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
