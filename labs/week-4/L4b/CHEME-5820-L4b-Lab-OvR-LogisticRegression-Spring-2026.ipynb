{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a39ab0",
   "metadata": {},
   "source": [
    "# L4b: One versus the Rest Strategy for Multi-class Classification\n",
    "In this lab, we implement the One versus the Rest (OvR) strategy, which extends binary logistic regression to multi-class problems by training one classifier per class. Each classifier learns to distinguish its assigned class from all others. OvR is computationally efficient and interpretable: rather than learning a complex multinomial model, we leverage multiple simple binary classifiers.\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this lab, you will be able to:\n",
    "> \n",
    "> * __Construct OvR datasets from MNIST:__ Convert one-hot labels into binary $\\pm 1$ labels for each digit and create balanced training/testing datasets.\n",
    "> * __Train one logistic regression model per class:__ Implement the OvR training loop and store a dictionary of trained binary classifiers.\n",
    "> * __Evaluate OvR performance with confusion matrices:__ Compute and interpret confusion matrices for a selected digit vs. the rest.\n",
    "\n",
    "Let's get started!\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f6a02",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f06385",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a78e46",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e7ad2",
   "metadata": {},
   "source": [
    "### Constants\n",
    "We define constants that control data loading, image dimensions, and the train/test split. The `number_of_examples::Int` constant sets how many images per digit to load from MNIST, while `number_of_test_examples::Int` reserves a portion for evaluation. The remaining constants specify the image geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73bb461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_examples = 3000; # how many training examples of *each* number to include from the library\n",
    "number_of_test_examples = 500; # how many examples are we going to test on?\n",
    "number_of_training_examples = number_of_examples - number_of_test_examples; # how many training examples of *each* number to include from the library\n",
    "number_digit_array = range(0,length=10,step=1) |> collect; # numbers 0 ... 9\n",
    "number_of_rows = 28; # number of rows in the image\n",
    "number_of_cols = 28; # number of cols in the image\n",
    "number_of_pixels = number_of_rows*number_of_cols; # how many pixels do we have in the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78e3a98",
   "metadata": {},
   "source": [
    "### Data\n",
    "Our data consists of images of handwritten digits (0–9) from the [Modified National Institute of Standards and Technology (MNIST) database](https://en.wikipedia.org/wiki/MNIST_database). \n",
    "\n",
    "We build a training dataset to estimate model parameters, stored in the `training_image_dataset::Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}` variable, and a test dataset to evaluate generalization to unseen data, stored in the `testing_image_dataset::Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}` variable.\n",
    "\n",
    "> __Data format:__ The first element of each tuple is the input data $\\mathbf{x}$ (the image pixels arranged as a vector), and the second element is the label (whether the image corresponds to digits 0–9).\n",
    ">\n",
    "> __Type considerations:__ The input data uses `Float64` precision for numerical stability in gradient-based optimization. The labels are [one-hot encoded](https://en.wikipedia.org/wiki/One-hot), and the input data is stored as a vector rather than a matrix (even though the original image is a $28\\times 28$ matrix of grayscale values).\n",
    "\n",
    "We load `number_of_examples::Int` images per digit into the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` dictionary, where each key is a digit (0–9) and each value is a 3D array of grayscale images. We then convert these to vector format by linearizing the $28\\times 28$ matrix of grayscale values into a vector of 784 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7691eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_image_dictionary = MyMNISTHandwrittenDigitImageDataset(number_of_examples = number_of_examples);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609940c",
   "metadata": {},
   "source": [
    "Let's inspect the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` variable. Each entry maps a digit to a $28\\times 28\\times n$ array, where $n$ is the number of images for that digit. We can index into the array to view individual images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df7f1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAABC9JREFUaAW9wV+oZHUdAPDPOec7c//srnI3a30LMiGicPFBkkhJqAgkhR6MpB7yIYgiFEkIgghBSAgDEyJIVnqpSB/yZS2FfMkHUYgkSRDBEHRdbu7de2fmzJxzGvgJs8Nc2rfv5xOuUGEHM3SKBh0COzhQVBhQK3pFoMLc8UKykCwkC0uhWOBIMUaDiWKBA1QYo1X0ii3sYl8xQmBiXUgWkoVkUWFh0xytYg8tDhFoMSh2cYQZesU25pjYFJKFZCFZjNAqTmCKDoOVfTSKAQNqDDjCGCMcKqYI7OLIupAsJAvJolU0aNEpapxEg31UioXiGhxirugU1+EiFljYFJKFZCFZWNrCDB0ajDDFJcUn8RBux42YYAeHOIE38AQew/v+v5AsJAvJwlJl5RQmio/ip/g8bsIFRa04gRluxEOo8WscokaDuXUhWUgWkoUr1JhihtP4Eb6B09jHeXwNHf6DFm/hNpzBF/BLRaC1KSQLyUKy2MFEsY0jxa14EC3+ggfwGs7iAG9hgcCDeAR3INChdryQLCQLyWJiZaa4BrdgjjG+rNjFq1YCC0wU/0armKJBZ11IFpKFZOEKveIyrkNgodjDvmIP+xihwk3ocIDBSmdTSBaShWTRoFOM0aLH66gQuBmvoEaPfYwwwfX4KhaYo8aAEVqbQrKQLCSLDjV6DBhjhr/hTXwCL+P7eBItBsxxCp/D9RjwEnpsoUeN3rqQLCQLycLSFiZo0WAb/8DP8ShO4WHcj7/jPTyLf+E+9DjA04oKc4zRWheShWQhWVgaUGFAh05xDiM8jD3sYQ8fwf2oMcMCv8Nril7R2hSShWQhWViaIlCjQ4czeBe/wpOY4xF8D5dxElOMMcOj6K006GwKyUKykCwsVeiwsHIBI8zRosM5fAdjDHgbH8O1+CN+hr9iihqBhXUhWUgWkoWlATUq9IoevWKOs3hV8QbO4ye4HX/AzTiHb+I8RpjZFJKFZCFZ+FClqDBYOY278Bu0eAH34R3F8/gSHsdn8RQ+jf86XkgWkoVk4UODYrDuTvwWF/FPfBfvKBpcxov4Mf6MLTyBe3AKB9aFZCFZSBaVYsCgqBBo8EPFu/g6LmILMzQ4gUt4Fk/hXnwc2ziwKSQLyUKyqDBgsFKhQeAGxeO4iJM4UrRoEdjDaTQ4g6njhWQhWUgWg6LCoBjQocLrOIvHcCt+gUt4EzvYw934AT6FQ7yCGr1NIVlIFpLFgAqVYsCABXrciefwGdyNe1HjRWzjFnyAk2jxNr6FbRzZFJKFZCFZWBowWDegw/v4Cu7At/FF7OA2DIoGL+BPeA5T9I4XkoVkIVm4im1cwO/xDGpMrQs0mFkJVJhbF5KFZCFZuIq5ldbKGLv4AA0q1OhRYeF4IVlIFpKFq+iwhRE6tOjQolXMrNtCjQ4z60KykCwk+x+gryLGGYa8IwAAAABJRU5ErkJggg==",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAABC9JREFUaAW9wV+oZHUdAPDPOec7c//srnI3a30LMiGicPFBkkhJqAgkhR6MpB7yIYgiFEkIgghBSAgDEyJIVnqpSB/yZS2FfMkHUYgkSRDBEHRdbu7de2fmzJxzGvgJs8Nc2rfv5xOuUGEHM3SKBh0COzhQVBhQK3pFoMLc8UKykCwkC0uhWOBIMUaDiWKBA1QYo1X0ii3sYl8xQmBiXUgWkoVkUWFh0xytYg8tDhFoMSh2cYQZesU25pjYFJKFZCFZjNAqTmCKDoOVfTSKAQNqDDjCGCMcKqYI7OLIupAsJAvJolU0aNEpapxEg31UioXiGhxirugU1+EiFljYFJKFZCFZWNrCDB0ajDDFJcUn8RBux42YYAeHOIE38AQew/v+v5AsJAvJwlJl5RQmio/ip/g8bsIFRa04gRluxEOo8WscokaDuXUhWUgWkoUr1JhihtP4Eb6B09jHeXwNHf6DFm/hNpzBF/BLRaC1KSQLyUKy2MFEsY0jxa14EC3+ggfwGs7iAG9hgcCDeAR3INChdryQLCQLyWJiZaa4BrdgjjG+rNjFq1YCC0wU/0armKJBZ11IFpKFZOEKveIyrkNgodjDvmIP+xihwk3ocIDBSmdTSBaShWTRoFOM0aLH66gQuBmvoEaPfYwwwfX4KhaYo8aAEVqbQrKQLCSLDjV6DBhjhr/hTXwCL+P7eBItBsxxCp/D9RjwEnpsoUeN3rqQLCQLycLSFiZo0WAb/8DP8ShO4WHcj7/jPTyLf+E+9DjA04oKc4zRWheShWQhWVgaUGFAh05xDiM8jD3sYQ8fwf2oMcMCv8Nril7R2hSShWQhWViaIlCjQ4czeBe/wpOY4xF8D5dxElOMMcOj6K006GwKyUKykCwsVeiwsHIBI8zRosM5fAdjDHgbH8O1+CN+hr9iihqBhXUhWUgWkoWlATUq9IoevWKOs3hV8QbO4ye4HX/AzTiHb+I8RpjZFJKFZCFZ+FClqDBYOY278Bu0eAH34R3F8/gSHsdn8RQ+jf86XkgWkoVk4UODYrDuTvwWF/FPfBfvKBpcxov4Mf6MLTyBe3AKB9aFZCFZSBaVYsCgqBBo8EPFu/g6LmILMzQ4gUt4Fk/hXnwc2ziwKSQLyUKyqDBgsFKhQeAGxeO4iJM4UrRoEdjDaTQ4g6njhWQhWUgWg6LCoBjQocLrOIvHcCt+gUt4EzvYw934AT6FQ7yCGr1NIVlIFpLFgAqVYsCABXrciefwGdyNe1HjRWzjFnyAk2jxNr6FbRzZFJKFZCFZWBowWDegw/v4Cu7At/FF7OA2DIoGL+BPeA5T9I4XkoVkIVm4im1cwO/xDGpMrQs0mFkJVJhbF5KFZCFZuIq5ldbKGLv4AA0q1OhRYeF4IVlIFpKFq+iwhRE6tOjQolXMrNtCjQ4z60KykCwk+x+gryLGGYa8IwAAAABJRU5ErkJg\">"
      ],
      "text/plain": [
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;235m▀\u001b[38;5;232;48;5;247m▀\u001b[38;5;232;48;5;253m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;233;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;253m▀\u001b[38;5;232;48;5;242m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;234;48;5;245m▀\u001b[38;5;246;48;5;255m▀\u001b[38;5;252;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;254;48;5;247m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;253m▀\u001b[38;5;255;48;5;252m▀\u001b[38;5;255;48;5;249m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;254;48;5;255m▀\u001b[38;5;243;48;5;249m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;239;48;5;244m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;250;48;5;232m▀\u001b[38;5;236;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;236;48;5;232m▀\u001b[38;5;235;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;243;48;5;242m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;251m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;246;48;5;242m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;232;48;5;236m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;247;48;5;254m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;250;48;5;250m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;236;48;5;232m▀\u001b[38;5;253;48;5;242m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;242;48;5;255m▀\u001b[38;5;232;48;5;253m▀\u001b[38;5;232;48;5;247m▀\u001b[38;5;232;48;5;245m▀\u001b[38;5;232;48;5;238m▀\u001b[38;5;233;48;5;234m▀\u001b[38;5;239;48;5;249m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;244;48;5;236m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;242;48;5;232m▀\u001b[38;5;255;48;5;233m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;245m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;246;48;5;235m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;236m▀\u001b[38;5;236;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;253m▀\u001b[38;5;255;48;5;246m▀\u001b[38;5;253;48;5;233m▀\u001b[38;5;255;48;5;239m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;251;48;5;255m▀\u001b[38;5;239;48;5;255m▀\u001b[38;5;232;48;5;238m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;241m▀\u001b[38;5;241;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;250m▀\u001b[38;5;249;48;5;234m▀\u001b[38;5;234;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;238;48;5;232m▀\u001b[38;5;252;48;5;239m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;254;48;5;255m▀\u001b[38;5;233;48;5;248m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;252;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;252;48;5;242m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;247;48;5;249m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;251;48;5;253m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;251;48;5;233m▀\u001b[38;5;255;48;5;252m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;247;48;5;255m▀\u001b[38;5;236;48;5;255m▀\u001b[38;5;235;48;5;255m▀\u001b[38;5;232;48;5;250m▀\u001b[38;5;232;48;5;251m▀\u001b[38;5;234;48;5;255m▀\u001b[38;5;242;48;5;255m▀\u001b[38;5;253;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;253m▀\u001b[38;5;248;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;234;48;5;232m▀\u001b[38;5;240;48;5;232m▀\u001b[38;5;249;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;250;48;5;232m▀\u001b[38;5;242;48;5;232m▀\u001b[38;5;235;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits_image_dictionary[8][ :, :, 10] # how does the indexing work? This is the 10th example of the digit \"8\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8b946",
   "metadata": {},
   "source": [
    "Next, let's partition the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` into training and testing datasets. We randomly select `number_of_training_examples::Int` images per digit for training, and the remaining images are used for testing. In each case, we convert the $28\\times 28$ images into vector format by linearizing the matrix into a vector of 784 pixels.\n",
    "\n",
    "Let's start with the training dataset. \n",
    "\n",
    "> __What is vectorization?__ Each $N\\times N$ image array containing grayscale values at each pixel is converted to an $N^{2}$ vector by concatenating pixel values. The image class (the digit it represents) is converted to [one-hot format](https://en.wikipedia.org/wiki/One-hot). \n",
    "\n",
    "Let's save the training data in the `training_image_dataset::Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43cd09f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000-element Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}:\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03137254901960784, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00784313725490196, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03529411764705882  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00784313725490196  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01568627450980392  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023529411764705882  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050980392156862744  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023529411764705882, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ⋮\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01568627450980392, 0.011764705882352941  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03529411764705882  …  0.01568627450980392, 0.00784313725490196, 0.011764705882352941, 0.0, 0.0, 0.0196078431372549, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.023529411764705882, 0.0, 0.0, 0.023529411764705882, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011764705882352941, 0.00784313725490196  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023529411764705882, 0.00392156862745098  …  0.0, 0.0, 0.07450980392156863, 0.050980392156862744, 0.0, 0.011764705882352941, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00784313725490196  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027450980392156862, 0.00392156862745098  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043137254901960784, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_image_dataset = let\n",
    "\n",
    "    # initialize -\n",
    "    training_image_dataset = Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}();\n",
    "    for i ∈ number_digit_array\n",
    "        Y = onehot(i, number_digit_array); # what image class is this?\n",
    "        X = digits_image_dictionary[i]; # this gets ALL images of digit \"i\"\n",
    "\n",
    "        for t ∈ 1:number_of_training_examples\n",
    "            D = reshape(transpose(X[:,:,t]) |> Matrix, number_of_pixels) |> vec; # flatten\n",
    "            training_tuple = (D,Y); # create training tuple (image data, image class)\n",
    "            push!(training_image_dataset,training_tuple);\n",
    "        end\n",
    "    end\n",
    "    training_image_dataset; # return\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497eaf9",
   "metadata": {},
   "source": [
    "Next, we load `number_of_test_examples::Int` images from the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` for testing purposes. We save the test data in the `testing_image_dataset::Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05c5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_image_dataset = let\n",
    "    \n",
    "    # initialize -\n",
    "    testing_image_dataset = Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}()\n",
    "    \n",
    "    # main -\n",
    "    for i ∈ number_digit_array\n",
    "        Y = onehot(i, number_digit_array); # what image class is this?\n",
    "        X = digits_image_dictionary[i]; # this gets ALL images of digit \"i\"\n",
    "        \n",
    "        for t ∈ (number_of_training_examples+1):number_of_examples\n",
    "\n",
    "            D = reshape(transpose(X[:,:,t]) |> Matrix, number_of_pixels) |> vec; # flatten\n",
    "            testing_tuple = (D,Y); # create testing tuple (image data, image class)\n",
    "            push!(testing_image_dataset, testing_tuple);\n",
    "        end\n",
    "    end\n",
    "\n",
    "    testing_image_dataset; # return\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d3de69",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Before constructing the OvR datasets, we define three helper functions that will be used across all tasks:\n",
    "\n",
    "> __Helper functions:__\n",
    ">\n",
    "> * `downsample_image(x::Vector{Float64}, pool::Int)::Vector{Float64}` reduces a 784-pixel image vector to a smaller feature vector by average pooling over non-overlapping $\\text{pool}\\times\\text{pool}$ blocks.\n",
    "> * `build_ovr_dataset(...)` takes the full image dataset and a target digit, then constructs a balanced binary dataset with $\\pm 1$ labels and downsampled features.\n",
    "> * `ovr_confusion_for_digit(digit::Int; ...)` evaluates a trained OvR classifier on test data and returns the confusion matrix, accuracy, predictions, true labels, and class probabilities.\n",
    "\n",
    "Let's define these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9025d0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ovr_confusion_for_digit (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function downsample_image(x::Vector{Float64}, pool::Int)::Vector{Float64}\n",
    "\n",
    "    # undo the earlier transpose used during vectorization\n",
    "    img = reshape(x, (number_of_cols, number_of_rows))';\n",
    "    new_rows = number_of_rows ÷ pool;\n",
    "    new_cols = number_of_cols ÷ pool;\n",
    "    pooled = zeros(Float64, new_rows, new_cols);\n",
    "\n",
    "    for i ∈ 1:new_rows\n",
    "        for j ∈ 1:new_cols\n",
    "            r = (pool*(i-1)+1):(pool*i);\n",
    "            c = (pool*(j-1)+1):(pool*j);\n",
    "            pooled[i,j] = mean(img[r,c]);\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return vec(pooled);\n",
    "end;\n",
    "\n",
    "function build_ovr_dataset(image_dataset::Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}},\n",
    "    digit::Int; number_positive::Int = 200, number_negative::Int = 200, pool::Int = 4)\n",
    "\n",
    "    # collect indices for positive/negative classes\n",
    "    pos_idx = Int[];\n",
    "    neg_idx = Int[];\n",
    "    for (i,(x,y_onehot)) ∈ enumerate(image_dataset)\n",
    "        label = number_digit_array[argmax(y_onehot)];\n",
    "        if (label == digit)\n",
    "            push!(pos_idx, i);\n",
    "        else\n",
    "            push!(neg_idx, i);\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # sample a balanced subset\n",
    "    pos_sel = pos_idx[randperm(length(pos_idx))[1:number_positive]];\n",
    "    neg_sel = neg_idx[randperm(length(neg_idx))[1:number_negative]];\n",
    "    selected = vcat(pos_sel, neg_sel);\n",
    "    selected = selected[randperm(length(selected))];\n",
    "\n",
    "    # allocate feature matrix and label vector\n",
    "    features_per_image = (number_of_rows ÷ pool) * (number_of_cols ÷ pool);\n",
    "    X = Array{Float64,2}(undef, length(selected), features_per_image);\n",
    "    y = Array{Int64,1}(undef, length(selected));\n",
    "\n",
    "    for (k, idx) ∈ enumerate(selected)\n",
    "        x, y_onehot = image_dataset[idx];\n",
    "        X[k,:] = downsample_image(x, pool);\n",
    "        label = number_digit_array[argmax(y_onehot)];\n",
    "        y[k] = (label == digit) ? 1 : -1;\n",
    "    end\n",
    "\n",
    "    return X, y;\n",
    "end;\n",
    "\n",
    "function ovr_confusion_for_digit(digit::Int;\n",
    "    dataset::Dict = ovr_testing_dataset,\n",
    "    models::Dict = ovr_model_dictionary)\n",
    "\n",
    "    @assert haskey(dataset, digit) \"Digit $(digit) not found in test dataset.\"\n",
    "    @assert haskey(models, digit) \"Digit $(digit) not found in model dictionary.\"\n",
    "\n",
    "    X_test, y_test = dataset[digit];\n",
    "    number_of_examples = size(X_test,1);\n",
    "    Xb = [X_test ones(number_of_examples)];\n",
    "    model = models[digit];\n",
    "\n",
    "    # compute class probabilities\n",
    "    P = classify(Xb, model);\n",
    "\n",
    "    # convert probabilities to ±1 labels\n",
    "    ŷ = zeros(Int64, number_of_examples);\n",
    "    for i ∈ 1:number_of_examples\n",
    "        ŷ[i] = (P[i,1] ≥ P[i,2]) ? 1 : -1;\n",
    "    end\n",
    "\n",
    "    CM = confusion(y_test, ŷ);\n",
    "    accuracy = (CM[1,1] + CM[2,2]) / length(y_test);\n",
    "\n",
    "    return CM, accuracy, ŷ, y_test, P\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9b1ac9",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ad8d8",
   "metadata": {},
   "source": [
    "## Task 1: Create the One versus the Rest (OvR) Dataset\n",
    "In this task, we create one binary dataset per digit. For a chosen digit `k`, we set the label to `+1` when the image shows digit `k`, and to `-1` otherwise.\n",
    "\n",
    "> __Implementation note__:\n",
    ">\n",
    "> The logistic regression learner in this package uses a finite-difference approximation of the gradient. This is computationally expensive for 784-pixel images. To keep the lab interactive, we:\n",
    ">\n",
    "> * Downsample each $28\\times 28$ image into a $7\\times 7$ average-pooled image (49 features).\n",
    "> * Build a balanced dataset with a small number of positive and negative examples per digit.\n",
    "\n",
    "We store the results in two dictionaries: `ovr_training_dataset` and `ovr_testing_dataset`. Each dictionary maps a digit to a tuple `(X, y)` where `X` is the feature matrix and `y` is the $\\pm 1$ label vector. These balanced datasets are the foundation for training individual classifiers in Task 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a716618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_training_dataset, ovr_testing_dataset = let\n",
    "\n",
    "    # initialize -\n",
    "    training_dict = Dict{Int, Tuple{Array{Float64,2}, Vector{Int64}}}();\n",
    "    testing_dict = Dict{Int, Tuple{Array{Float64,2}, Vector{Int64}}}();\n",
    "\n",
    "    # options -\n",
    "    pool = 4;\n",
    "    ovr_training_positive = 200;\n",
    "    ovr_training_negative = 200;\n",
    "    ovr_testing_positive = 100;\n",
    "    ovr_testing_negative = 100;\n",
    "\n",
    "    # build datasets for each digit\n",
    "    for digit ∈ number_digit_array\n",
    "        Xtr, ytr = build_ovr_dataset(training_image_dataset, digit,\n",
    "            number_positive = ovr_training_positive,\n",
    "            number_negative = ovr_training_negative,\n",
    "            pool = pool\n",
    "        );\n",
    "        Xte, yte = build_ovr_dataset(testing_image_dataset, digit,\n",
    "            number_positive = ovr_testing_positive,\n",
    "            number_negative = ovr_testing_negative,\n",
    "            pool = pool\n",
    "        );\n",
    "        training_dict[digit] = (Xtr, ytr);\n",
    "        testing_dict[digit] = (Xte, yte);\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    training_dict, testing_dict\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e8a3b",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb4a178",
   "metadata": {},
   "source": [
    "## Task 2: Train an OvR Logistic Regression Model for Each Digit\n",
    "In this task, we train one binary logistic regression model per digit using the OvR datasets constructed in Task 1. For each digit, we fit a separate classifier that learns to recognize that digit versus all others. We keep the trained models in the `ovr_model_dictionary::Dict{Int, MyLogisticRegressionClassificationModel}` dictionary.\n",
    "\n",
    "> __Training loop__: For each digit, we add a bias term to the feature matrix, build a `MyLogisticRegressionClassificationModel`, and learn the parameters using gradient descent.\n",
    "\n",
    "For time-limited scenarios, you can train once and save the models to disk for later use.\n",
    "\n",
    "> __Offline training option__:\n",
    ">\n",
    "> You can train once, save the model dictionary to disk, and reload it during the lab session.\n",
    "> If you change the downsampling factor (`pool`) or the dataset sizes, you should retrain and re-save.\n",
    "\n",
    "Let's train the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd50c534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: type Main.#27#28 does not exist in workspace; reconstructing\n",
      "└ @ JLD2 /Users/jeffreyvarner/.julia/packages/JLD2/hbsZG/src/data/reconstructing_datatypes.jl:472\n"
     ]
    },
    {
     "ename": "MethodError",
     "evalue": "MethodError: Cannot `convert` an object of type JLD2.ReconstructedSingleton{Symbol(\"#27#28\")} to an object of type Function\nThe function `convert` exists, but no method is defined for this combination of argument types.\n\nClosest candidates are:\n  convert(::Type{T}, !Matched::T) where T\n   @ Base Base_compiler.jl:133\n",
     "output_type": "error",
     "traceback": [
      "MethodError: Cannot `convert` an object of type JLD2.ReconstructedSingleton{Symbol(\"#27#28\")} to an object of type Function\n",
      "The function `convert` exists, but no method is defined for this combination of argument types.\n",
      "\n",
      "Closest candidates are:\n",
      "  convert(::Type{T}, !Matched::T) where T\n",
      "   @ Base Base_compiler.jl:133\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "  [1] rconvert(T::Type, x::JLD2.ReconstructedSingleton{Symbol(\"#27#28\")})\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/data/custom_serialization.jl:11\n",
      "  [2] jlconvert(::JLD2.MappedRepr{Function, JLD2.RelOffset}, f::JLD2.JLDFile{JLD2.MmapIO}, ptr::Ptr{Nothing}, ::JLD2.RelOffset)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/data/writing_datatypes.jl:339\n",
      "  [3] macro expansion\n",
      "    @ ~/.julia/packages/JLD2/hbsZG/src/data/reconstructing_datatypes.jl:671 [inlined]\n",
      "  [4] jlconvert(::JLD2.MappedRepr{MyLogisticRegressionClassificationModel, JLD2.OnDiskRepresentation{(0, 8, 16, 24, 32, 40, 48), Tuple{Vector{Float64}, Float64, Float64, Function, Float64, Float64, Float64}, Tuple{JLD2.RelOffset, Float64, Float64, JLD2.RelOffset, Float64, Float64, Float64}, 56}}, f::JLD2.JLDFile{JLD2.MmapIO}, ptr::Ptr{Nothing}, header_offset::JLD2.RelOffset)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/data/reconstructing_datatypes.jl:663\n",
      "  [5] read_scalar(f::JLD2.JLDFile{JLD2.MmapIO}, rr::Any, header_offset::JLD2.RelOffset)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/io/dataio.jl:107\n",
      "  [6] read_data(f::JLD2.JLDFile{JLD2.MmapIO}, rr::Any, read_dataspace::Tuple{JLD2.ReadDataspace, JLD2.RelOffset, JLD2.DataLayout, JLD2.Filters.FilterPipeline{Tuple{}}}, attributes::Vector{JLD2.ReadAttribute})\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/datasets.jl:96\n",
      "  [7] read_data(f::JLD2.JLDFile{JLD2.MmapIO}, dataspace::JLD2.ReadDataspace, dt::JLD2.H5Datatype, layout::JLD2.DataLayout, filters::JLD2.Filters.FilterPipeline{Tuple{}}, header_offset::JLD2.RelOffset, attributes::Vector{JLD2.ReadAttribute})\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/datasets.jl:84\n",
      "  [8] load_dataset(f::JLD2.JLDFile{JLD2.MmapIO}, offset::JLD2.RelOffset)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/datasets.jl:48\n",
      "  [9] jlconvert(::JLD2.MappedRepr{MyLogisticRegressionClassificationModel, JLD2.RelOffset}, f::JLD2.JLDFile{JLD2.MmapIO}, ptr::Ptr{Nothing}, ::JLD2.RelOffset)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/data/writing_datatypes.jl:338\n",
      " [10] macro expansion\n",
      "    @ ~/.julia/packages/JLD2/hbsZG/src/data/reconstructing_datatypes.jl:710 [inlined]\n",
      " [11] jlconvert(::JLD2.MappedRepr{Pair{Int64, MyLogisticRegressionClassificationModel}, JLD2.OnDiskRepresentation{(0, 8), Tuple{Int64, MyLogisticRegressionClassificationModel}, Tuple{Int64, JLD2.RelOffset}, 16}}, f::JLD2.JLDFile{JLD2.MmapIO}, ptr::Ptr{Nothing}, header_offset::JLD2.RelOffset)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/data/reconstructing_datatypes.jl:663\n",
      " [12] read_scalar(f::JLD2.JLDFile{JLD2.MmapIO}, rr::Any, header_offset::JLD2.RelOffset)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/io/dataio.jl:107\n",
      " [13] read_data(f::JLD2.JLDFile{JLD2.MmapIO}, rr::Any, read_dataspace::Tuple{JLD2.ReadDataspace, JLD2.RelOffset, JLD2.DataLayout, JLD2.Filters.FilterPipeline{Tuple{}}}, attributes::Vector{JLD2.ReadAttribute})\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/datasets.jl:96\n",
      " [14] read_data(f::JLD2.JLDFile{JLD2.MmapIO}, dataspace::JLD2.ReadDataspace, dt::JLD2.H5Datatype, layout::JLD2.DataLayout, filters::JLD2.Filters.FilterPipeline{Tuple{}}, header_offset::JLD2.RelOffset, attributes::Vector{JLD2.ReadAttribute})\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/datasets.jl:84\n",
      " [15] load_dataset(f::JLD2.JLDFile{JLD2.MmapIO}, offset::JLD2.RelOffset)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/datasets.jl:48\n",
      " [16] jlconvert(::JLD2.MappedRepr{Pair{Int64, MyLogisticRegressionClassificationModel}, JLD2.RelOffset}, f::JLD2.JLDFile{JLD2.MmapIO}, ptr::Ptr{Nothing}, ::JLD2.RelOffset)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/data/writing_datatypes.jl:338\n",
      " [17] macro expansion\n",
      "    @ ~/.julia/packages/JLD2/hbsZG/src/io/dataio.jl:125 [inlined]\n",
      " [18] macro expansion\n",
      "    @ ./simdloop.jl:77 [inlined]\n",
      " [19] read_array!(v::Vector{Pair{Int64, MyLogisticRegressionClassificationModel}}, f::JLD2.JLDFile{JLD2.MmapIO}, rr::JLD2.MappedRepr{Pair{Int64, MyLogisticRegressionClassificationModel}, JLD2.RelOffset})\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/io/dataio.jl:122\n",
      " [20] read_array(f::JLD2.JLDFile{JLD2.MmapIO}, dataspace::JLD2.ReadDataspace, rr::JLD2.ReadRepresentation, layout::JLD2.DataLayout, filters::JLD2.Filters.FilterPipeline{Tuple{}}, header_offset::JLD2.RelOffset, attributes::Vector{JLD2.ReadAttribute})\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/datasets.jl:248\n",
      " [21] read_data(f::JLD2.JLDFile{JLD2.MmapIO}, ::JLD2.MappedRepr{Any, JLD2.RelOffset}, read_dataspace::Tuple{JLD2.ReadDataspace, JLD2.RelOffset, JLD2.DataLayout, JLD2.Filters.FilterPipeline{Tuple{}}}, attributes::Vector{JLD2.ReadAttribute})\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/datasets.jl:136\n",
      " [22] read_data(f::JLD2.JLDFile{JLD2.MmapIO}, dataspace::JLD2.ReadDataspace, dt::JLD2.H5Datatype, layout::JLD2.DataLayout, filters::JLD2.Filters.FilterPipeline{Tuple{}}, header_offset::JLD2.RelOffset, attributes::Vector{JLD2.ReadAttribute})\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/datasets.jl:84\n",
      " [23] load_dataset(f::JLD2.JLDFile{JLD2.MmapIO}, offset::JLD2.RelOffset)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/datasets.jl:48\n",
      " [24] jlconvert\n",
      "    @ ~/.julia/packages/JLD2/hbsZG/src/data/writing_datatypes.jl:338 [inlined]\n",
      " [25] macro expansion\n",
      "    @ ~/.julia/packages/JLD2/hbsZG/src/data/reconstructing_datatypes.jl:671 [inlined]\n",
      " [26] jlconvert\n",
      "    @ ~/.julia/packages/JLD2/hbsZG/src/data/reconstructing_datatypes.jl:663 [inlined]\n",
      " [27] jlconvert(::JLD2.MappedRepr{Dict{Int64, MyLogisticRegressionClassificationModel}, JLD2.CustomSerialization{JLD2.SerializedDict, JLD2.OnDiskRepresentation{(0,), Tuple{Any}, Tuple{JLD2.RelOffset}, 8}}}, f::JLD2.JLDFile{JLD2.MmapIO}, ptr::Ptr{Nothing}, header_offset::JLD2.RelOffset)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/data/custom_serialization.jl:75\n",
      " [28] read_scalar(f::JLD2.JLDFile{JLD2.MmapIO}, rr::Any, header_offset::JLD2.RelOffset)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/io/dataio.jl:107\n",
      " [29] read_data(f::JLD2.JLDFile{JLD2.MmapIO}, rr::Any, read_dataspace::Tuple{JLD2.ReadDataspace, JLD2.RelOffset, JLD2.DataLayout, JLD2.Filters.FilterPipeline{Tuple{}}}, attributes::Vector{JLD2.ReadAttribute})\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/datasets.jl:96\n",
      " [30] read_data(f::JLD2.JLDFile{JLD2.MmapIO}, dataspace::JLD2.ReadDataspace, dt::JLD2.H5Datatype, layout::JLD2.DataLayout, filters::JLD2.Filters.FilterPipeline{Tuple{}}, header_offset::JLD2.RelOffset, attributes::Vector{JLD2.ReadAttribute})\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/datasets.jl:84\n",
      " [31] load_dataset(f::JLD2.JLDFile{JLD2.MmapIO}, offset::JLD2.RelOffset)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/datasets.jl:48\n",
      " [32] getindex(g::JLD2.Group{JLD2.JLDFile{JLD2.MmapIO}}, name::String)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/groups.jl:99\n",
      " [33] read\n",
      "    @ ~/.julia/packages/JLD2/hbsZG/src/JLD2.jl:368 [inlined]\n",
      " [34] (::var\"#26#27\")(f::JLD2.JLDFile{JLD2.MmapIO})\n",
      "    @ Main ~/.julia/packages/JLD2/hbsZG/src/loadsave.jl:153\n",
      " [35] jldopen(f::Function, args::String; kws::@Kwargs{})\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/loadsave.jl:4\n",
      " [36] jldopen(f::Function, args::String)\n",
      "    @ JLD2 ~/.julia/packages/JLD2/hbsZG/src/loadsave.jl:1\n",
      " [37] top-level scope\n",
      "    @ ~/Desktop/julia_work/CHEME-5820-instances/Spring-2026/CHEME-5820-Labs-Spring-2026/labs/week-4/L4b/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y233sZmlsZQ==.jl:10\n",
      " [38] macro expansion\n",
      "    @ ~/.julia/packages/JLD2/hbsZG/src/loadsave.jl:152 [inlined]"
     ]
    }
   ],
   "source": [
    "ovr_model_dictionary = let\n",
    "\n",
    "    # initialize -\n",
    "    digits_to_train = number_digit_array; # change this if you want to train fewer digits\n",
    "    models_file = joinpath(_PATH_TO_DATA, \"ovr_logistic_models.jld2\");\n",
    "    use_pretrained = true; # set to false to force retraining\n",
    "    train_if_missing = true; # if pretrained file is missing, should we train?\n",
    "\n",
    "    if (use_pretrained == true && isfile(models_file))\n",
    "        @load models_file ovr_model_dictionary\n",
    "        ovr_model_dictionary\n",
    "    elseif (train_if_missing == true)\n",
    "\n",
    "        # training hyperparameters\n",
    "        maxiter = 150;\n",
    "        ϵ = 1e-3;\n",
    "        α = 0.05;\n",
    "        h = 1e-6;\n",
    "        T = 1.0;\n",
    "        λ = 0.0;\n",
    "\n",
    "        model_dictionary = Dict{Int, MyLogisticRegressionClassificationModel}();\n",
    "\n",
    "        for digit ∈ digits_to_train\n",
    "            X, y = ovr_training_dataset[digit];\n",
    "            number_of_examples = size(X,1);\n",
    "            Xb = [X ones(number_of_examples)];\n",
    "            number_of_features = size(Xb,2);\n",
    "\n",
    "            loss_function = (x,y,T,λ,θ) -> log(1+exp(-2*y*T*(dot(x,θ)))) + λ*norm(θ,2)^2;\n",
    "\n",
    "            model = build(MyLogisticRegressionClassificationModel, (\n",
    "                parameters = 0.01*ones(number_of_features),\n",
    "                learning_rate = α,\n",
    "                ϵ = ϵ,\n",
    "                h = h,\n",
    "                λ = λ,\n",
    "                T = T,\n",
    "                loss_function = loss_function\n",
    "            ));\n",
    "\n",
    "            trained = learn(Xb, y, model, maxiter = maxiter, verbose = true);\n",
    "            model_dictionary[digit] = trained;\n",
    "        end\n",
    "\n",
    "        # save for offline use\n",
    "        mkpath(_PATH_TO_DATA);\n",
    "        ovr_model_dictionary = model_dictionary;\n",
    "        @save models_file ovr_model_dictionary\n",
    "\n",
    "        model_dictionary\n",
    "    else\n",
    "        error(\"No pretrained models found at $(models_file). Set `use_pretrained = false` to train.\");\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188750a1",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93034ea7",
   "metadata": {},
   "source": [
    "## Task 3: Evaluate One Digit vs. Rest Using a Confusion Matrix\n",
    "In this task, we validate the trained classifiers from Task 2 by selecting a digit and evaluating its OvR classifier on the test dataset. We compute predicted labels from the probability matrix and construct a confusion matrix to assess performance.\n",
    "\n",
    "> __Reminder__: In this OvR setting, the “positive” class means “is the digit,” and the “negative” class means “is not the digit.”\n",
    "\n",
    "Now let's select a digit and evaluate its classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af2ec8ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `ovr_model_dictionary` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `ovr_model_dictionary` not defined in `Main`\n",
      "Suggestion: add an appropriate import or assignment. This global was declared but not assigned.\n",
      "\n",
      "Stacktrace:\n",
      " [1] ovr_confusion_for_digit(digit::Int64)\n",
      "   @ Main ~/Desktop/julia_work/CHEME-5820-instances/Spring-2026/CHEME-5820-Labs-Spring-2026/labs/week-4/L4b/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y224sZmlsZQ==.jl:56\n",
      " [2] top-level scope\n",
      "   @ ~/Desktop/julia_work/CHEME-5820-instances/Spring-2026/CHEME-5820-Labs-Spring-2026/labs/week-4/L4b/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y236sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "digit_to_test = 8; # change this to evaluate a different digit\n",
    "CM_ovr, accuracy_ovr, ŷ_ovr, y_ovr, P_ovr = ovr_confusion_for_digit(digit_to_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3628a",
   "metadata": {},
   "source": [
    "The `ovr_confusion_for_digit` function returns the confusion matrix `CM_ovr::Matrix{Int64}`, the overall accuracy `accuracy_ovr::Float64`, the predicted labels `ŷ_ovr::Vector{Int64}`, the true labels `y_ovr::Vector{Int64}`, and the probability matrix `P_ovr::Matrix{Float64}`. Let's recompute and display the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94e6cf64",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `y_ovr` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `y_ovr` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Desktop/julia_work/CHEME-5820-instances/Spring-2026/CHEME-5820-Labs-Spring-2026/labs/week-4/L4b/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y240sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "CM_ovr = confusion(y_ovr, ŷ_ovr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0f130",
   "metadata": {},
   "source": [
    "From the confusion matrix, we can compute the fraction of correctly classified test examples. The diagonal entries `CM_ovr[1,1]` and `CM_ovr[2,2]` count the true positives and true negatives, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec45f65b",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `y_ovr` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `y_ovr` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Desktop/julia_work/CHEME-5820-instances/Spring-2026/CHEME-5820-Labs-Spring-2026/labs/week-4/L4b/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y241sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "number_of_test_points = length(y_ovr);\n",
    "correct_prediction = CM_ovr[1,1] + CM_ovr[2,2];\n",
    "(correct_prediction/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "The OvR strategy decomposes multi-class classification into independent binary problems, enabling the direct application of binary logistic regression to multi-digit recognition.\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> * __OvR decomposes multi-class into binary problems:__ Each digit gets its own classifier that distinguishes that digit from all others.\n",
    "> * __Dimensionality control matters in practice:__ Downsampling and balanced subsets make gradient-based training feasible while preserving the OvR workflow.\n",
    "> * __Confusion matrices reveal digit-specific behavior:__ Evaluating one digit vs. rest highlights false positives and false negatives for that digit.\n",
    "\n",
    "This simple, scalable approach provides a foundation for extending binary classifiers to multi-class settings.\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.4",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
